+ RNN with ADAM, is very **sensitive to the learning rate**; in most of the conditions, it learned with a lr greater than 0.0001. 
  + Only in one condition it learned: 

+ GRU is more robust to the optimizer (since with the same architecture as RNN, it could learn with 0.001 learning rate)

+ 
